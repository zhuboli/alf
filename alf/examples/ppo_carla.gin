include 'ppo.gin'
include 'carla.gin'
import alf.algorithms.encoding_algorithm

encoding_dim = 256
fc_layers_params = (256,)
activation = torch.relu_

actor/StableNormalProjectionNetwork.state_dependent_std=True
actor/StableNormalProjectionNetwork.squash_mean=False
actor/StableNormalProjectionNetwork.scale_distribution=True
actor/StableNormalProjectionNetwork.min_std=1e-3
actor/StableNormalProjectionNetwork.max_std=10.
actor/StableNormalProjectionNetwork.dist_squashing_transform=@dist_utils.Softsign()
actor/ActorDistributionNetwork.fc_layer_params=%fc_layers_params
actor/ActorDistributionNetwork.activation=%activation
actor/ActorDistributionNetwork.use_fc_bn=%use_batch_normalization
actor/ActorDistributionNetwork.continuous_projection_net_ctor=@actor/StableNormalProjectionNetwork

value/ValueNetwork.fc_layer_params=%fc_layers_params
value/ValueNetwork.activation=%activation
value/ValueNetwork.use_fc_bn=%use_batch_normalization

PPOLoss.gamma=0.99
PPOLoss.normalize_advantages=True
PPOLoss.td_lambda=0.95
PPOLoss.td_error_loss_fn=@element_wise_squared_loss
PPOLoss.check_numerics=True
#PPOLoss.entropy_regularization=0.01

PPOAlgorithm.actor_network_ctor=@actor/ActorDistributionNetwork
PPOAlgorithm.value_network_ctor=@value/ValueNetwork

ac/AdamTF.lr=2e-4
encoder/EncodingNetwork.input_preprocessors=%input_preprocessors
encoder/EncodingNetwork.preprocessing_combiner=@NestSum(activation=%activation, average=True)
encoder/EncodingNetwork.activation=%activation
encoder/EncodingNetwork.use_fc_bn=%use_batch_normalization
encoder/EncodingNetwork.fc_layer_params=%fc_layers_params
encoder/EncodingAlgorithm.encoder_cls=@encoder/EncodingNetwork
Agent.observation_transformer=@agent/image_scale_transformer
Agent.representation_learner_cls=@encoder/EncodingAlgorithm
Agent.optimizer=@ac/AdamTF()
agent/image_scale_transformer.min=0.0
agent/image_scale_transformer.fields=['observation.camera']

agent/EntropyTargetAlgorithm.initial_alpha=1.
agent/EntropyTargetAlgorithm.skip_free_stage=True
agent/EntropyTargetAlgorithm.slow_update_rate=1e-3
agent/EntropyTargetAlgorithm.fast_update_rate=1e-3
Agent.entropy_target_cls=@agent/EntropyTargetAlgorithm
use_batch_normalization=True

# Not yet able to successfully train with sparse reward.
suite_carla.Player.sparse_reward=False
suite_carla.Player.sparse_reward_interval=2.0

# Currently, even a small penalty such as one make the training much worse
suite_carla.Player.max_collision_penalty=100.

create_environment.num_parallel_environments=32

# training config
TrainerConfig.mini_batch_length=1
TrainerConfig.unroll_length=128
TrainerConfig.mini_batch_size=256
TrainerConfig.num_iterations=10000
TrainerConfig.num_checkpoints=20
TrainerConfig.num_updates_per_train_iter=16
TrainerConfig.eval_interval=1000
TrainerConfig.debug_summaries=True
TrainerConfig.summarize_grads_and_vars=True
TrainerConfig.summarize_action_distributions=True
TrainerConfig.summary_interval=1


